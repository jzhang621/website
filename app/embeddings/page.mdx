import RemoveWord from '@/components/embeddings/RemoveWord';
import CustomImage from '@/components/Image';
import Image from 'next/image';
import { pg_remove_words_results } from '@/data/words/pg_remove_words_results';

import Wattenberger from '../../public/images/wattenberger.png';
import ChatGPT from '../../public/images/chat-gpt-transform.png';
import Brighten from '../../public/images/brighten.png';
import Gray from '../../public/images/gray.png';
import GrayScale from '../../public/images/gray-scale.png';
import BrightenSingle from '../../public/images/brighten-single.png';
import GraySingle from '../../public/images/gray-single.png';
import Prism from '../../public/images/prism-toolbar.png';
import Red from '../../public/images/red.png';
import Apple from '../../public/images/apple.png';

import ColorSwatch from '@/components/rgb/ColorSwatch';
import { generateRGBGradient } from '@/data/utils';

export const metadata = {
    published: true,
    title: "Why I'm Excited About Embeddings",
    description: "We can now write functions that allow us to work with text in new ways.",
    date: "2024-12-03",
};

export const grays = generateRGBGradient([ 90, 90, 90], [180, 180, 180], 3);

<h1 className="text-4xl font-bold">{metadata.title}</h1>
<div className="mt-2 text-lg text-gray-500">{metadata.description}</div>

In 2018, I tried to visualize what good writing looks like. My goal was to write a program that could take in a piece of writing, and return some dimensions of that writing that could be used to discern its quality.


My first attempts were really basic. I was dealing with a lot of lengths: lengths of sentences, lengths of words, etc, which I plotted as bar charts and histograms. I didn't find these plots to be particularly interesting, and I lost interest in the project. 

But *embeddings* have me excited about this idea again!

An embedding is a <span className="italic">numerical representation</span> of text. Instead of representing text as a sequence of characters (a string), embeddings represent text as a sequence of numbers. These numbers encode a lot of information, but before we get into what that information is, I want to first show why numerical representations are important by looking at how computers represent color.



## RGB Color Model

tl,dr: Having an accurate numerical representation of color facilitates us to create powerful facilitate such as Instagram Filters.

The [RGB color model](https://www.britannica.com/science/RGB-color-model) is a way of describing colors as 24-bit numbers (for a total of 2^24 or ~16.7 million unique colors). That 24-bit number is broken down into 3 separate 8-bit dimensions. Each dimension corresponds to an intensity of red (R), green (G), and blue (B) light known as *channels* that range from 0 to 255.

If we have a RGB color and want to make it "more red", we have an easy way of doing so in terms of its underlying representation - we increase the red channel while keeping the other channels constant. 

<div className="my-12">
<div className="flex justify-center">
```typescript
function moreRed(color: [number, number, number], amount: number) {
	const [r, g, b] = color;
	return [r + amount, g, b];
}
```
</div>
<figcaption className="mt-8 text-center text-sm text-gray-500">Note: this function should clamp all values between 0 and 255, but I'm excluding that for the sake of clarity.</figcaption>
</div>

<div className="flex justify-center h-auto mx-auto w-3/5 my-8">
  <Image src={Red} 
    alt="Brightening a single color"
    width={0}
    height={0}
    sizes="100vw"
    className="w-full h-auto"
  />
</div>

We can express more complex transformations just as easily. For example:


#### Brightness

To brighten<sup>1</sup> a color, we can increase the value of each channel by the same relative amount. 

<div className="flex justify-center my-12">
```javascript
function brighten(color: [number, number, number], amount: number) {
	const [r, g, b] = color;
	return [r * amount, g * amount, b * amount];
}
```
</div>

We can see the effects of this transformation when it is applied to a single color.

<div className="flex justify-center h-auto mx-auto w-3/5 my-8">
  <Image src={BrightenSingle}
    alt="Brightening a single color"
    width={0}
    height={0}
    sizes="100vw"
    className="w-full h-auto"
  />
</div>


These effects are even more apparent when we apply the transformation to an entire image:

<div className="flex justify-center h-auto mx-auto w-3/5 my-8">
  <Image src={Brighten} 
    alt="Brightening a single color"
    width={0}
    height={0}
    sizes="100vw"
    className="w-full h-auto"
  />
</div>


#### Grayness

In the RGB color model, colors with an equal intensity across all three channels (i.e. `r = g = b`) are all different shades of gray:

<div className="flex gap-8 justify-center my-8 py-4 rounded-sm">
{grays.map((color, index) => (
 <ColorSwatch rgb={color} key={index} />
))}
</div>

This means we can turn an input color "gray" by calculating the average of its three channels, and returning a new color where all channels have that average. 

<div className="flex justify-center my-12">
```javascript
function gray(color: [number, number, number]) {
	const [r, g, b] = color;
    const avg = (r + g + b) / 3;
	return [avg, avg, avg];
}
```
</div>

<div className="flex justify-center  h-auto mx-auto w-3/5 my-8">
  <Image src={GraySingle} 
    alt="Turning a single color gray"
    width={0}
    height={0}
    sizes="100vw"
    className="w-full h-auto"
  />
</div>

When applied to each pixel of an image:

<div className="flex justify-center  h-auto mx-auto w-3/5 my-8">
  <Image src={Gray} 
    alt="Graying an entire image"
    width={0}
    height={0}
    sizes="100vw"
    className="w-full h-auto"
  />
</div>


The RGB color model works because it is rooted in reality: it's based on the [Trichromatic Theory of Color Vision](https://www.simplypsychology.org/what-is-the-trichromatic-theory-of-color-vision.html). In other words, it's a numerical representation that reflects how humans actually perceive color (to some degree of accuracy). **This makes the RGB color model an enabler** - it enhances what we can do with colors by enabling us to define functions that transform color like the ones shown above.


And you're familiar with these color transformation functions already - they're the basis of Instagram filters.

Note: This idea of "representations as enablers" comes from [Synthesizers for Thought](https://thesephist.com/posts/synth/) by Linus Lee.

## Embeddings

The RGB Color Model is a useful frame for understanding embeddings. Just like the RGB color model is a numerical representation of color based on how humans actually perceive color, an embedding is a numerical representation of text based on humans actually use text in writing. Similarly, embeddings enhance what we can do with text.

To create an embedding, you take a piece of text and pass it through an *embedding model*. You can create embeddings of all types of text - individual words, phrases, paragraphs, even entire documents - but we'll stick to sentences in this post.

```python
model.encode(”This is a sentence I will generate an embedding for.”) → [1234, …, …, …]
```

You can take any sentence you can think of and run it through the embedding model - the output is always a list of numbers with length `N`. We refer to `N` as the *dimension* of the embedding. 


```python
model.encode(”My cat is being a trouble maker today.”) → [1234, …, …, …]
```


We can think of a single embedding as occupying a point in an N-dimensional space (an embedding is a *vector*). An embedding model thus maps all possible sentences onto points in the same N-dimensional space. Here's the most important part: they are mapped in such a way that **sentences with “similar meanings” sit closer together in this space** than sentences with different meanings.

```python
[need spatial visual here]

model.encode(”I want to generate an embedding for this sentence.”) → [1234, …, …, …]

model.encode(”I want to get an embedding for this sentence.”) → [1234, …, …, …]

model.encode(”My cat is being a trouble maker today.”) → [1234, …, …, …]
```

Since embeddings are just points in the same N-dimensional space, we can use *cosine similarity* to measure the distance between any two embeddings. Cosine similarity is a function that measures how similar two vectors are to each other by measuring the angle between them. When used in the context of embeddings, cosine similarity can be thought of as a way of "measuring meaning": if two sentences are related in meaning, their cosine similarity will be closer to 1. If they are unrelated, their cosine similarity will be closer to 0.


### Computing with Embeddings

Cosine similarity brings us back to the main idea of this post: that embeddings enhance what we can do with text.

For example, here's the start of a Paul Graham essay, [When to Do What You Love](https://www.paulgraham.com/when.html), with certain words highlighted with the help of embeddings and cosine similarity:

<div className="m-8">
    <RemoveWord words={pg_remove_words_results} />
</div>

To create these highlights, I first generate embeddings for each sentence in the essay `(e1)`. I then loop through every word in each sentence, remove that word from the sentence, and generate an embedding for that shortened sentence `(e2)`.

I then calculate the cosine similarity between `e1` and `e2`, and use that value to drive the highlighting. Words, that when removed, result in larger differences in cosine similarity, are highlighted darker. The result is a rough measure of how “important” a word is to the meaning of the sentence in which that word appears. 

This technique has its shortcomings. For one, long sentences tend to dilute the importance of any single word. And it'd probably be more informative to remove entire phrases such as "follow your passion" in some places rather than individual words. Even so, I find it's a helpful indicator of where to direct my attention. I like the idea of turning these highlights on after reading something for the first time, as I find it prompts me to read each sentence more closely.

But its not the virtues of any particular technique that excite me the most - it's the fact that such techniques are even possible. Here are two other embeddings-based techinques for working with text that others have imagined, which go beyond using embeddings to measure meaning.

#### Measuring Different Scales

Below, [Amelia Wattenberger](https://wattenberger.com/thoughts/yay-embeddings-math) demonstrates how embeddings can be used to measure sentences on a scale of “concrete” to “abstract”. (I <span className="italic">especially love</span> the view on the right-hand side which plots the entire essay along that scale).

<div className="my-12">
<div className="flex justify-center">
  <Image 
    src={Wattenberger} 
    alt="Description" 
    />
</div>
<figcaption className="mt-8 text-center text-sm text-gray-500">Credit: Amelia Wattenberger (https://wattenberger.com/thoughts/yay-embeddings-math)</figcaption>
</div>

#### Instagram Filters, but for Text?
Just as the RGB color model facilitates meaningful transforms of color, embeddings also facilitate meaningful transformations of text. 


We can already do this with ChatGPT when we ask it to make a sentence more "concrete":

<div className="flex justify-center my-12">
<Image 
    src={ChatGPT} 
    alt="Transforming text with ChatGPT" 
    />
</div>

And this is because under the hood, the Large Language Models (LLMs) that power ChatGPT are just manipulating embeddings!

Right now, we use natural language to interface with the LLM, which manipulate the embeddings in response to our prompts. But more natural interfaces - ones that manipulate embeddings more precisely - like the text editing interface Linus Lee [imagines below](https://thesephist.com/posts/prism/) are possible:

<div className="my-12">
<div className="flex justify-center">
  <Image 
    src={Prism} 
    alt="Description" 
    height={500}
    />
</div>
<figcaption className="mt-4 text-center text-sm text-gray-500">Credit: Linus Lee (https://thesephist.com/posts/prism/)</figcaption>
</div>


And what's even more exciting is that embeddings can be generalized to all different types of data. We can create embeddings for images, songs, videos, and even abstract concepts like one's movie taste (these different types of data are called *modalities*). All embeddings, regardless of the modality, follow the same principle: similar data sit close together in the embedding space. This means that images with similar content, songs with similar sounds, or movies with similar themes will occupy nearby points in their respective spaces.

We can even create *multi-modal embeddings*, where different modalities, such as images and text, are mapped (or aligned) to the same embedding space. This is the underlying idea behind Generative AI applications, which enable us to generate images based on a text descriptions ("a red apple on a wooden table"), or the reverse, in which we provide an image, and generate a text caption based on its contents.

<div className="my-12">
<div className="flex justify-center">
  <Image 
    src={Apple} 
    alt="Description" 
    height={300}
    />
</div>
</div>


So while I'm personally most excited about text embeddings, embeddings are truly at the heart of the recent advancements in machine learning. And in order to get a more complete understanding of how embeddings work and the transforms they enable, we have to take a closer look at embedding models, which we'll do in the next post.

Stay tuned!