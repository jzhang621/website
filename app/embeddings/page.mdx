import RemoveWord from '@/components/embeddings/RemoveWord';
import CustomImage from '@/components/Image';
import Image from 'next/image';
import { pg_remove_words_results } from '@/data/words/pg_remove_words_results';

import Wattenberger from '../../public/images/wattenberger.png';
import ChatGPT from '../../public/images/chat-gpt-transform.png';
import Brighten from '../../public/images/brighten.png';
import Gray from '../../public/images/gray.png';
import GrayScale from '../../public/images/gray-scale.png';
import BrightenSingle from '../../public/images/brighten-single.png';
import GraySingle from '../../public/images/gray-single.png';
import Prism from '../../public/images/prism-toolbar.png';
import Red from '../../public/images/red.png';
import Apple from '../../public/images/apple.png';

import ColorTransform from '@/components/embeddings/ColorTransform';
import { EmbeddingVisualizer } from '@/components/embeddings/Visualizer';
import { e1, e2, e3, diffArrays, findGlobalMinMax } from '@/components/embeddings/data';

import ColorSwatch from '@/components/rgb/ColorSwatch';
import { generateRGBGradient } from '@/data/utils';
import CanvasImage from '@/components/embeddings/CanvasImage';

export const metadata = {
    published: true,
    title: "Why I'm Excited About Embeddings",
    description: "We can now write functions that allow us to work with text in new ways.",
    date: "2024-12-03",
};

export const [min, max] = findGlobalMinMax([e1, e2, e3]);

export const [min2, max2] = findGlobalMinMax([diffArrays(e1, e2), diffArrays(e1, e3)]);

export const grays = generateRGBGradient([ 90, 90, 90], [180, 180, 180], 3);

<h1 className="text-4xl font-bold">{metadata.title}</h1>
<div className="mt-2 text-lg text-gray-500">{metadata.description}</div>

In 2018, I tried to visualize what good writing looks like. My goal was to write a program that could take in a piece of writing, and return some dimensions of that writing that could be used to discern its quality.


My first attempts were really basic. I was dealing with a lot of lengths: lengths of sentences, lengths of words, etc, which I plotted as bar charts and histograms. I didn't find these plots to be particularly interesting, and I lost interest in the project. 

But *embeddings* have me excited about this idea again!

An embedding is a <span className="italic">numerical representation</span> of text. Instead of representing text as a sequence of characters (a string), embeddings represent text as a sequence of numbers. These numbers enable us to do some interesting things, but before we get into what those are, I want to first show why numerical representations are important by looking at how computers represent color.



## RGB Color Model

The [RGB color model](https://www.britannica.com/science/RGB-color-model) is a way of describing colors as 24-bit numbers (for a total of 2^24 or ~16.7 million unique colors). That 24-bit number is broken down into 3 separate 8-bit dimensions. Each dimension corresponds to an intensity of red (R), green (G), and blue (B) light known as *channels* that range from 0 to 255.

If we have a RGB color and want to make it "more red", we have an easy way of doing so in terms of its underlying representation - we increase the red channel while keeping the other channels constant. 

<div className="my-12">
<div className="flex justify-center">
```typescript
function moreRed(color: [number, number, number], amount: number) {
	const [r, g, b] = color;
	return [r + amount, g, b];
}
```
</div>
<figcaption className="mt-8 text-center text-sm text-gray-500">Note: this function should clamp all values between 0 and 255, but I'm excluding that for the sake of clarity.</figcaption>
</div>


<div className="flex justify-center w-4/5 mx-auto">
<ColorTransform
  startColor={[150, 0, 0]}
  endColor={[250, 0, 0]}
  functionLabel="moreRed([150, 0, 0], 100)"
/>
</div>

We can express more complex transformations just as easily. For example:


#### Brightness

To brighten<sup>1</sup> a color, we can increase the value of each channel by the same relative amount. 

<div className="flex justify-center my-12">
```javascript
function brighten(color: [number, number, number], amount: number) {
	const [r, g, b] = color;
	return [r * amount, g * amount, b * amount];
}
```
</div>

We can see the effects of this transformation when it is applied to a single color.

<div className="flex justify-center w-4/5 mx-auto">
<ColorTransform
  startColor={[140, 60, 90]}
  endColor={[210, 90, 150]}
  functionLabel="brighten([140, 60, 90], 1.5)"
/>
</div>

These effects are even more apparent when we apply the transformation to an entire image:

<div className="flex justify-center my-12 w-4/5 mx-auto">
<CanvasImage 
    imageUrl="/images/redwood.png"
    otherImageUrl="/images/brighteness.png"
    functionLabel="brighten(..., 1.5)"
/>
</div>



#### Grayness

In the RGB color model, colors with an equal intensity across all three channels (i.e. `r = g = b`) are all different shades of gray:

<div className="flex gap-8 justify-center my-8 py-4 rounded-sm">
{grays.map((color, index) => (
 <ColorSwatch rgb={color} key={index} />
))}
</div>

This means we can turn an input color "gray" by calculating the average of its three channels, and returning a new color where all channels have that average. 

<div className="flex justify-center my-12">
```javascript
function gray(color: [number, number, number]) {
	const [r, g, b] = color;
    const avg = (r + g + b) / 3;
	return [avg, avg, avg];
}
```
</div>

<div className="flex justify-center w-4/5 mx-auto">
<ColorTransform
  startColor={[140, 60, 90]}
  endColor={[96, 96, 96]}
  functionLabel="gray([140, 60, 90])"
/>
</div>


When applied to each pixel of an image:

<div className="flex justify-center my-12 w-4/5 mx-auto">
<CanvasImage 
    imageUrl="/images/redwood.png"
    otherImageUrl="/images/black-and-white.png"
    functionLabel="gray(...)"
/>
</div>


The RGB color model works because it is rooted in reality: it's based on the [Trichromatic Theory of Color Vision](https://www.simplypsychology.org/what-is-the-trichromatic-theory-of-color-vision.html). In other words, it's a numerical representation that reflects (to some degree of accuracy) how humans actually perceive color. **This makes the RGB color model an enabler** - it enhances what we can do with colors by enabling us to define functions that transform color like the ones shown above.

You're familiar with these color transformation functions already - they're the basis of Instagram filters.

Note: This idea of "representations as enablers" comes from [Synthesizers for Thought](https://thesephist.com/posts/synth/) by Linus Lee.

## Embeddings

The RGB Color Model is a useful frame for understanding embeddings. Just like the RGB color model is a numerical representation of color based on how humans actually perceive color, an embedding is a numerical representation of text based on humans actually use text in writing. Similarly, embeddings enhance what we can do with text.

To create an embedding, you take a piece of text and pass it through an *embedding model*. You can create embeddings of all types of text - individual words, phrases, paragraphs, even entire documents - but we'll stick to sentences in this post. The embedding model takes in the sentence as input, and returns a list of numbers (the numeric representation of the sentence).

```python
model.encode("I will generate an embedding for this sentence.")

# array([-4.16772217e-02,  2.16492657e-02,  4.84335013e-02,  7.46034831e-02,
#        4.97868052e-03,  8.82944185e-03,  1.26351800e-03, -1.59129705e-02,
#        1.37419058e-02, -6.40833601e-02,  4.88254726e-02, -6.41501993e-02,
#        6.66038692e-02, -2.81214025e-02, -1.42913852e-02,  4.82733026e-02,
#        5.97095154e-02,  1.00764818e-02, -6.96659312e-02, -1.83627214e-02,
#       -1.26159436e-03,  6.00421168e-02,  9.91466492e-02, -6.49396032e-02,
#       -1.34562636e-02, -3.08830962e-02,  2.45258817e-03,  8.54900777e-02,
#        9.22604650e-02, -1.16281090e-02,  9.16429088e-02, -3.64247337e-02,
#        7.14990078e-03,  6.29822165e-03,  5.78497946e-02,  3.67692038e-02,
#       -1.34286843e-02,  4.29249555e-02,  6.40979037e-02, -2.83057783e-02,
#        2.58675008e-03, -5.78473322e-02,  2.40517855e-02,  9.08062086e-02,
#        3.68111506e-02, -5.10181710e-02, -2.69073006e-02,  9.82946344e-03,
#       -5.20526953e-02,  2.62302700e-02, -5.46806790e-02, -6.50678799e-02,
#       -2.51216199e-02, -1.17777273e-01, -4.37260568e-02, -1.46411443e-02,
#       -1.99552290e-02, -2.39421465e-02,  3.25184390e-02, -6.62487596e-02,
#        1.39292600e-02, -7.73431733e-03, -8.75708275e-03,  4.29031216e-02,
#        6.35881573e-02,  1.17423162e-02,  4.17440236e-02,  7.28243962e-02,
#       -5.78757413e-02,  8.25852975e-02, -5.29911667e-02,  1.30494814e-02,
#       -8.36730674e-02,  6.50826283e-03, -1.24487393e-01, -1.97989456e-02,
#       -5.77960443e-03, -7.39938989e-02,  1.47626966e-01, -4.51644184e-03,
#       -1.03832679e-02, -4.86191660e-02, -4.79090251e-02,  3.16649079e-02,
#        1.17653385e-02, -1.07086520e-03,  1.21311896e-01, -8.90002549e-02,
#       -6.87703714e-02,  1.90233707e-03, -7.72426277e-02, -6.88459128e-02,
#        2.96560396e-02, -5.68634272e-03, -8.66875723e-02,  4.81836163e-02,
#       -9.27904323e-02, -3.61349322e-02,  1.70177519e-02,  7.73033127e-02,
# ...
#        1.46252392e-02,  4.24624458e-02,  1.64152845e-03, -5.83416894e-02,
#        8.33863243e-02, -1.46255363e-02,  3.98303606e-02,  8.90543498e-03,
#       -9.75205656e-03, -1.94375291e-02,  7.32998131e-03,  5.72182946e-02,
#        3.28265540e-02,  6.62673712e-02,  4.29528579e-02, -5.50971180e-02],
# dtype=float32)
```

<div className="my-8 mx-8 mx-auto">
<EmbeddingVisualizer embedding={e1} min={min} max={max} id="e1" />
</div>

You can take any sentence you can think of and run it through the embedding model - the output is always a list of numbers with length `N`. We refer to `N` as the *dimension* of the embedding model. The dimension of our embedding model is 384.


```python
model.encode("My cat is being a trouble maker today.")
```

{/* <div className="my-8 mx-8 mx-auto">
<EmbeddingVisualizer embedding={e2} min={min} max={max} id="e2" />
</div> */}

Below: embedding visualizer for the sentence "I will get an embedding for this sentence."

{/* <div className="my-8 mx-8 mx-auto">
<EmbeddingVisualizer embedding={e3} min={min} max={max} id="e3" />
</div> */}

Below: embedding visualizer for the difference between the cat sentence and the sentence "I will get an embedding for this sentence."

<div className="my-8 mx-8 mx-auto">
<EmbeddingVisualizer embedding={diffArrays(e1, e2)} min={min2} max={max2} abs={true} id="diff-e1-e2" />
</div>

Below: embedding visualizer for the difference between "I will generate an embedding for this sentence." and "I will get an embedding for this sentence." (less difference)

<div className="my-8 mx-8 mx-auto">
<EmbeddingVisualizer embedding={diffArrays(e1, e3)} min={min2} max={max2} abs={true} id="diff-e1-e3" />
</div>




An embedding is a *vector*, meaning we can think of it as occupying a point in an N-dimensional space. An embedding model thus maps all possible sentences onto points in the same N-dimensional space. Here's the most important part: they are mapped in such a way that **sentences with “similar meanings” sit closer together in this space** than sentences with different meanings.

```python
model.encode("I want to generate an embedding for this sentence.") → [1234, …, …, …]

model.encode("I want to get an embedding for this sentence.") → [1234, …, …, …]

model.encode("My cat is being a trouble maker today.") → [1234, …, …, …]
```

Since embeddings are just points in the same N-dimensional space, we can use *cosine similarity* to measure the distance between any two embeddings. Cosine similarity is a function that measures how similar two vectors are to each other by measuring the angle between them. When used in the context of embeddings, cosine similarity can be thought of as a way of "measuring meaning": if two sentences are related in meaning, their cosine similarity will be closer to 1. If they are unrelated, their cosine similarity will be closer to 0.


### Computing with Embeddings

Cosine similarity brings us back to the main idea of this post: that embeddings enhance what we can do with text.

For example, here's the start of a Paul Graham essay, [When to Do What You Love](https://www.paulgraham.com/when.html), with certain words highlighted with the help of embeddings and cosine similarity:

<div className="m-8">
    <RemoveWord words={pg_remove_words_results} />
</div>

To create these highlights, I first generate embeddings for each sentence in the essay `(e1)`. I then loop through every word in each sentence, remove that word from the sentence, and generate an embedding for that shortened sentence `(e2)`.

I then calculate the cosine similarity between `e1` and `e2`, and use that value to drive the highlighting. Words, that when removed, result in larger differences in cosine similarity, are highlighted darker. The result is a rough measure of how “important” a word is to the meaning of the sentence in which that word appears. 

This technique has its shortcomings. For one, long sentences tend to dilute the importance of any single word. And it'd probably be more informative to remove entire phrases such as "follow your passion" in some places rather than individual words. Even so, I find it's a helpful indicator of where to direct my attention. I like the idea of turning these highlights on after reading something for the first time, as I find it prompts me to read each sentence more closely.

But its not the virtues of any particular technique that excite me the most - it's the fact that such techniques are even possible. Here are two other embeddings-based techinques for working with text that others have imagined, which go beyond using embeddings to measure meaning.

#### Measuring Different Scales

Below, [Amelia Wattenberger](https://wattenberger.com/thoughts/yay-embeddings-math) demonstrates how embeddings can be used to measure sentences on a scale of “concrete” to “abstract”. (I <span className="italic">especially love</span> the view on the right-hand side which plots the entire essay along that scale).

<div className="my-12">
<div className="flex justify-center">
  <Image 
    src={Wattenberger} 
    alt="Description" 
    />
</div>
<figcaption className="mt-8 text-center text-sm text-gray-500">Credit: Amelia Wattenberger (https://wattenberger.com/thoughts/yay-embeddings-math)</figcaption>
</div>

#### Instagram Filters, but for Text?
Just as the RGB color model facilitates meaningful transforms of color, embeddings also facilitate meaningful transformations of text. 


We can already do this with ChatGPT when we ask it to make a sentence more "concrete":

<div className="flex justify-center my-12">
<Image 
    src={ChatGPT} 
    alt="Transforming text with ChatGPT" 
    />
</div>

And this is because under the hood, the Large Language Models (LLMs) that power ChatGPT are just manipulating embeddings!

Right now, we use natural language to interface with the LLM, which manipulate the embeddings in response to our prompts. But more natural interfaces - ones that manipulate embeddings more precisely - like the text editing interface Linus Lee [imagines below](https://thesephist.com/posts/prism/) are possible:

<div className="my-12">
<div className="flex justify-center">
  <Image 
    src={Prism} 
    alt="Description" 
    height={500}
    />
</div>
<figcaption className="mt-4 text-center text-sm text-gray-500">Credit: Linus Lee (https://thesephist.com/posts/prism/)</figcaption>
</div>


And what's even more exciting is that embeddings can be generalized to all different types of data. We can create embeddings for images, songs, videos, and even abstract concepts like one's movie taste (these different types of data are called *modalities*). All embeddings, regardless of the modality, follow the same principle: similar data sit close together in the embedding space. This means that images with similar content, songs with similar sounds, or movies with similar themes will occupy nearby points in their respective spaces.

We can even create *multi-modal embeddings*, where different modalities, such as images and text, are mapped (or aligned) to the same embedding space. This is the underlying idea behind Generative AI applications, which enable us to generate images based on a text descriptions ("a red apple on a wooden table"), or the reverse, in which we provide an image, and generate a text caption based on its contents.

<div className="my-12">
<div className="flex justify-center">
  <Image 
    src={Apple} 
    alt="Description" 
    height={300}
    />
</div>
</div>


So while I'm personally most excited about text embeddings, embeddings are truly at the heart of the recent advancements in machine learning. And in order to get a more complete understanding of how embeddings work and the transforms they enable, we have to take a closer look at embedding models, which we'll do in the next post.

Stay tuned!